import numpy as np
from sklearn.datasets import make_classification
from torch.utils.data.distributed import DistributedSampler

from base import *

class DataSet(torch.utils.data.Dataset):
    X: np.ndarray
    y: np.ndarray

    def __len__(self):
        return len(self.y)
    
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

def smaller() -> DataSet:
    # Create a smaller dataset for testing with 100 float features and a
    # binary classification label. The dtype of the data is given in base.py.
    n_features = 100
    data = make_classification(
        n_samples=100000, n_features=n_features,
        n_informative=n_features - 2, n_redundant=2,
        n_classes=2, weights=[0.98, 0.02],
        random_state=123
    )

    ds = DataSet()
    ds.X = data[0].astype(float_np)
    ds.y = data[1].astype(float_np)
    return ds


def prepare(rank, world_size, batch_size=batch_size, pin_memory=True, num_workers=0):
    # Prepares a Distributed Sampler (to distribute the data to different GPUs) and
    # dataloader for the dataset generated by smaller().
    dataset = smaller()
    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank, shuffle=False, drop_last=False)
    
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,
                                               pin_memory=pin_memory, num_workers=num_workers,
                                               drop_last=False, shuffle=False, sampler=sampler)
    return dataloader